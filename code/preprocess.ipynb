{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Stage\n",
    "\n",
    "### Some of the code used in this notebook make reference to the Spark documentation\n",
    "https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "### and Akira's tutorial\n",
    "\n",
    "https://github.com/akiratwang/MAST30034_Python/blob/main/advanced_tutorials/Spark%20Tutorial.ipynb"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# make schema\n",
    "\n",
    "columns = ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', \n",
    "        'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', \n",
    "        'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', \n",
    "        'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n",
    "ints = ('VendorID', 'passenger_count', 'RatecodeID', 'PULocationID', 'DOLocationID', \n",
    "        'payment_type')\n",
    "doubles = ('trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', \n",
    "           'improvement_surcharge', 'total_amount', 'congestion_surcharge')\n",
    "strings = ('store_and_fwd_flag',)\n",
    "dtimes = ('tpep_pickup_datetime', 'tpep_dropoff_datetime')\n",
    "dtypes = {column: IntegerType() for column in ints}\n",
    "dtypes.update({column: DoubleType() for column in doubles})\n",
    "dtypes.update({column: StringType() for column in strings})\n",
    "dtypes.update({column: TimestampType() for column in dtimes})\n",
    "\n",
    "schema = StructType()\n",
    "\n",
    "\n",
    "for column in columns:\n",
    "    schema.add(column, \n",
    "               dtypes[column],\n",
    "               True )\n",
    "\n",
    "\n",
    "sdf = spark.read.csv(\"../raw_data/2019\", header=True, schema=schema) \\\n",
    "        .withColumnRenamed(\"RatecodeID\",\"RateCodeID\") \\\n",
    "        .withColumnRenamed('tpep_pickup_datetime', 'pickup_time') \\\n",
    "        .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_time')\n",
    "\n",
    "sdf.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- pickup_time: timestamp (nullable = true)\n",
      " |-- dropoff_time: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RateCodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "f\"{sdf.count():,} rows\"\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'84,399,019 rows'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "sdf = sdf.withColumn(\"Month\", F.month(\"pickup_time\")) \\\n",
    "    .withColumn(\"hour\", F.hour(\"pickup_time\")) \\\n",
    "    .withColumn(\"dayofmonth\", F.dayofmonth(\"pickup_time\")) \\\n",
    "    .withColumn(\"weekday\", F.dayofweek(\"pickup_time\")) \\\n",
    "    .withColumn(\"duration\", F.unix_timestamp(\"dropoff_time\") - F.unix_timestamp(\"pickup_time\"))\n",
    "\n",
    "\n",
    "sdf = sdf.drop(\"pickup_time\", \"dropoff_time\", \"store_and_fwd_flag\", \"VendorID\")\n",
    "\n",
    "# only look at trips paid with cash or card, remove \"dispute\",\"voided trips\", etc.\n",
    "sdf = sdf.filter((sdf.payment_type == 1) | (sdf.payment_type == 2))\n",
    "# according to the fare directory\n",
    "sdf = sdf.filter(sdf.fare_amount > 2.5)\n",
    "# according to the lookup csv zone 264 and 265 are unknown zones\n",
    "sdf = sdf.filter((sdf.PULocationID != 264) | (sdf.DOLocationID != 264))\n",
    "sdf = sdf.filter((sdf.PULocationID != 265) | (sdf.DOLocationID != 265))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "## Try to find any missing values\n",
    "# sdf.where(col(\"total_amount\").isNull())\n",
    "# sdf.where(col(\"trip_distance\").isNull())\n",
    "## didn't find any missing values in fact"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aggregation starts from here\n",
    "### The results need for future will be saved into parquet file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from pyspark.sql.functions import sum, mean\n",
    "# aggregat the results needed for future analysis\n",
    "pickup_results = sdf.groupBy('PULocationID','Month', 'weekday', 'hour') \\\n",
    "        .agg(sum(\"fare_amount\").alias(\"sum_fare_amount\"),\n",
    "             mean(\"fare_amount\").alias(\"avg_fare_amount\"),\n",
    "             sum(\"trip_distance\").alias(\"total_trip_distance\"),\n",
    "             sum(\"duration\").alias(\"total_duration\"))\n",
    "# pickup_results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# save the results to the disk as a parquet file\n",
    "from shutil import rmtree\n",
    "from os import path\n",
    "\n",
    "fpath = '../data/aggregated_results1.parquet/'\n",
    "if path.exists(fpath):\n",
    "    rmtree(fpath)\n",
    "pickup_results.write.format('parquet').save('../data/aggregated_results1.parquet')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/08/17 02:11:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "21/08/17 02:11:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "21/08/17 02:11:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "21/08/17 02:11:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "21/08/17 02:11:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "21/08/17 02:11:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 56.31% for 12 writers\n",
      "21/08/17 02:11:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "21/08/17 02:11:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "21/08/17 02:11:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "21/08/17 02:11:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "21/08/17 02:11:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "21/08/17 02:11:42 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "byhour_results = sdf.groupBy('PULocationID','DOLocationID', 'hour').count()\n",
    "fpath = '../data/aggregated_results2.parquet/'\n",
    "if path.exists(fpath):\n",
    "    rmtree(fpath)\n",
    "byhour_results.write.format('parquet').save(fpath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "results_for_temp = sdf.groupBy('PULocationID','Month','dayofmonth') \\\n",
    "            .agg(sum(\"fare_amount\").alias(\"sum_fare_amount\"),\n",
    "                 mean(\"fare_amount\").alias(\"avg_fare_amount\"),\n",
    "                 sum(\"trip_distance\").alias(\"total_trip_distance\"),\n",
    "                 sum(\"duration\").alias(\"total_duration\"))\n",
    "fpath = '../data/aggregated_results3.parquet/'\n",
    "if path.exists(fpath):\n",
    "    rmtree(fpath)\n",
    "results_for_temp.write.format('parquet').save(fpath)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}